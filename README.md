# Jules ğŸ—£ï¸ğŸ¤–

An AI-powered chatbot built with LangChain + LangGraph on a FastAPI backend and a Next.js (React, TypeScript, Material-UI) frontend.

## Project layout

```
.
â”œâ”€â”€ backend/          # FastAPI + LangChain services
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â””â”€â”€ routers/
â”‚   â”‚       â””â”€â”€ chat.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ frontend/         # Next.js (TypeScript) client
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ next.config.js
â”‚   â”œâ”€â”€ tsconfig.json
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â””â”€â”€ index.tsx
â”‚   â”‚   â””â”€â”€ components/
â”‚   â”‚       â””â”€â”€ Chat.tsx
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ .env              # OpenAI key and config (never commit)
```

## Quick start (development)

1. Copy `.env.example` â†’ `.env` and add your `OPENAI_API_KEY`.
2. Install Python deps & run backend:

```bash
cd backend
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
uvicorn app.main:app --reload --port 8000
```

Conversation memory is stored in-process using a singleton MemorySaver, so it
persists only while the server is running.

Then open http://localhost:8000 to chat with **Jules**.
The backend will return the generated `X-Thread-ID` header on the very first
request so that clients can persist it.  Subsequent calls should repeat the
same ID either via the `X-Thread-ID` header or as a `thread_id` query
parameter to continue the conversation thread.

## Docker (single image)

```bash
docker compose up --build
```

Open your browser:

â€¢ http://localhost:3000  â†’ Next.js dev UI with hot-reload
â€¢ http://localhost:8000  â†’ FastAPI API (and the static build when dev server is stopped)

## License

MIT
